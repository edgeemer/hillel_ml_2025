{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qfPGQJoPE8gf",
   "metadata": {
    "id": "qfPGQJoPE8gf"
   },
   "outputs": [],
   "source": [
    "!pip install datasets > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ff67e2-7f3d-47fb-b6dd-b795fd5d4007",
   "metadata": {
    "id": "90ff67e2-7f3d-47fb-b6dd-b795fd5d4007"
   },
   "outputs": [],
   "source": [
    "import sys  # –º–æ–¥—É–ª—å –¥–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ —Å–∏—Å—Ç–µ–º–æ—é\n",
    "from typing import List, Tuple, Mapping  # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö\n",
    "\n",
    "import datasets  # –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –¥–∞–Ω–∏–º–∏\n",
    "from tqdm import tqdm  # –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É\n",
    "import numpy as np  # –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –º–∞—Å–∏–≤–∞–º–∏ –¥–∞–Ω–∏—Ö\n",
    "import matplotlib.pyplot as plt  # –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó\n",
    "\n",
    "import torch  # –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂\n",
    "import torch.nn as nn  # –º–æ–¥—É–ª—å –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂\n",
    "import torch.optim as optim  # –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∏\n",
    "import torch.nn.functional as F  # —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó –Ω–∞–¥ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏\n",
    "from torch.nn.utils.rnn import pad_sequence  # –¥–ª—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π\n",
    "from torch.utils.data import Dataset, DataLoader  # –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –¥–∞–Ω–∏–º–∏ —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ó—Ö —É –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tidOXR2lFKvY",
   "metadata": {
    "id": "tidOXR2lFKvY"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "**[HuggingFace](https://huggingface.co/datasets/benjamin/ner-uk)** <br>\n",
    "**[GitHub](https://github.com/lang-uk/ner-uk/tree/master)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87a917-3b53-4a0e-a53c-0a024210afba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f87a917-3b53-4a0e-a53c-0a024210afba",
    "outputId": "7d25fb49-2c6d-4d02-9be1-1c3dfd2d9260"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"benjamin/ner-uk\")  # –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é\n",
    "dataset  # –≤–∏–≤–µ–¥–µ–Ω–Ω—è –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02c955-d66f-46d5-8c7f-f14edbf7256d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef02c955-d66f-46d5-8c7f-f14edbf7256d",
    "outputId": "e1aa6e49-0d67-41a9-d84b-2437440200da"
   },
   "outputs": [],
   "source": [
    "# NOTE: –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –¥–ª—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –≤—Ö—ñ–¥–Ω–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π.\n",
    "TOK_PAD_ID = 0\n",
    "\n",
    "vocab = {\"<PAD>\": TOK_PAD_ID}  # —Å–ª–æ–≤–Ω–∏–∫, –ø–æ—á–∏–Ω–∞—î–º–æ –∑ —ñ–Ω–¥–µ–∫—Å—É –¥–ª—è –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è\n",
    "curr_idx = 1  # –ø–æ—Ç–æ—á–Ω–∏–π —ñ–Ω–¥–µ–∫—Å –¥–ª—è —ñ–Ω–¥–µ–∫—Å—É–≤–∞–Ω–Ω—è —Å–ª—ñ–≤\n",
    "for split in (\"train\", \"validation\", \"test\"):  # –ø—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è, –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è\n",
    "    for sample in dataset[split]:  # –ø—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –∫–æ–∂–Ω–æ–º—É –∑—Ä–∞–∑–∫—É –≤ –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö\n",
    "        for word in sample[\"tokens\"]:  # –ø—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –∫–æ–∂–Ω–æ–º—É —Å–ª–æ–≤—É –≤ —Ç–æ–∫–µ–Ω–∞—Ö –∑—Ä–∞–∑–∫—É\n",
    "            if word not in vocab:  # —è–∫—â–æ —Å–ª–æ–≤–æ —â–µ –Ω–µ –∑—É—Å—Ç—Ä—ñ—á–∞–ª–æ—Å—è\n",
    "                vocab[word] = curr_idx  # –¥–æ–¥–∞—î–º–æ –π–æ–≥–æ –¥–æ —Å–ª–æ–≤–Ω–∏–∫–∞ –∑ –ø–æ—Ç–æ—á–Ω–∏–º —ñ–Ω–¥–µ–∫—Å–æ–º\n",
    "                curr_idx += 1  # –∑–±—ñ–ª—å—à—É—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —ñ–Ω–¥–µ–∫—Å\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))  # –≤–∏–≤–æ–¥–∏–º–æ —Ä–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de5b35-f19e-486f-99e9-9f71f24d5098",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38de5b35-f19e-486f-99e9-9f71f24d5098",
    "outputId": "111ec69f-4a88-42aa-ab94-cf6370f7bea7"
   },
   "outputs": [],
   "source": [
    "# NOTE: –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ —Ü–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –º—ñ—Ç–æ–∫,\n",
    "#       —É —Ñ—É–Ω–∫—Ü—ñ—ó CrossEntropyLoss —î —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä, —è–∫–∏–π –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è 'ignore_index'\n",
    "#       —ñ –≤—ñ–Ω —ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏–º–µ –º—ñ—Ç–∫–∏ –∑ —Ü–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º (–æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç\n",
    "#       –±—É–¥–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ –¥–ª—è –º—ñ—Ç–æ–∫, —è–∫—ñ –¥–æ—Ä—ñ–≤–Ω—é—é—Ç—å —Ü—å–æ–º—É –∑–Ω–∞—á–µ–Ω–Ω—é)\n",
    "NER_PAD_ID = -100\n",
    "\n",
    "targets = set()  # –º–Ω–æ–∂–∏–Ω–∞ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –º—ñ—Ç–æ–∫ —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π\n",
    "for split in (\"train\", \"validation\", \"test\"):  # –ø—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è, –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è\n",
    "    for sample in dataset[split]:  # –ø—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –∫–æ–∂–Ω–æ–º—É –∑—Ä–∞–∑–∫—É –≤ –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö\n",
    "        targets.update(sample[\"ner_tags\"])  # –¥–æ–¥–∞—î–º–æ —É—Å—ñ –º—ñ—Ç–∫–∏ –∑—Ä–∞–∑–∫—É –¥–æ –º–Ω–æ–∂–∏–Ω–∏\n",
    "\n",
    "targets = sorted(targets)  # —Å–æ—Ä—Ç—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –º—ñ—Ç–∫–∏\n",
    "print(\"Unique targets:\", len(targets))  # –≤–∏–≤–æ–¥–∏–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –º—ñ—Ç–æ–∫\n",
    "targets  # –≤–∏–≤–æ–¥–∏–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –º—ñ—Ç–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5745311f-5198-4ccc-9985-e3b9f0289bac",
   "metadata": {
    "id": "5745311f-5198-4ccc-9985-e3b9f0289bac"
   },
   "source": [
    "## PyTorch Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a15fa0-7962-4d7f-9dd0-a95300b2ca30",
   "metadata": {
    "id": "08a15fa0-7962-4d7f-9dd0-a95300b2ca30"
   },
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, samples: datasets.Dataset, vocabulary: Mapping[str, int]) -> None:\n",
    "        self.samples = samples  # –∑—Ä–∞–∑–∫–∏ –¥–∞–Ω–∏—Ö\n",
    "        self.vocabulary = vocabulary  # —Å–ª–æ–≤–Ω–∏–∫ —Ç–æ–∫–µ–Ω—ñ–≤ —Ç–∞ —ó—Ö —ñ–Ω–¥–µ–∫—Å—ñ–≤\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)  # –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ —É –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        sample = self.samples[index]  # –≤–∏–±–∏—Ä–∞—î–º–æ –∑—Ä–∞–∑–æ–∫ –∑–∞ —ñ–Ω–¥–µ–∫—Å–æ–º\n",
    "        doc = torch.LongTensor([self.vocabulary[token] for token in sample[\"tokens\"]])  # —ñ–Ω–¥–µ–∫—Å—É—î–º–æ —Ç–æ–∫–µ–Ω–∏ –∑—Ä–∞–∑–∫–∞\n",
    "        label = torch.LongTensor(sample[\"ner_tags\"])  # –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –º—ñ—Ç–∫–∏ —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π —É —Ç–µ–Ω–∑–æ—Ä\n",
    "        return doc, label  # –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–µ–Ω–∑–æ—Ä —Ç–æ–∫–µ–Ω—ñ–≤ —Ç–∞ –º—ñ—Ç–æ–∫ —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23ddfe-9768-4a1d-bc81-51e8c140ccb1",
   "metadata": {
    "id": "fd23ddfe-9768-4a1d-bc81-51e8c140ccb1"
   },
   "outputs": [],
   "source": [
    "def seq_collate_fn(\n",
    "    batch: List[Tuple[torch.LongTensor, torch.LongTensor]], data_pad: int, label_pad: int\n",
    ") -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"Combine samples into batch that can be used laten by RNN model.\n",
    "\n",
    "    Args:\n",
    "        batch: list with tensors that should be packed into batch.\n",
    "            Expected that each list sample will be a tuple of (text_tokens, label_tokens).\n",
    "        data_pad: value to use for padding text tokens.\n",
    "        label_pad: value to use for padding label tokens.\n",
    "\n",
    "    Returns:\n",
    "        Padded and packed into batch text tokens and padded and packed into batch label tokens.\n",
    "    \"\"\"\n",
    "    token_ids = pad_sequence([item[0] for item in batch], batch_first=True, padding_value=data_pad)\n",
    "    label_ids = pad_sequence([item[1] for item in batch], batch_first=True, padding_value=label_pad)\n",
    "    return token_ids, label_ids\n",
    "\n",
    "\n",
    "def ner_collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]]) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"Collator function for our NER dataset.\n",
    "\n",
    "    Args:\n",
    "        batch: list with tensors that should be packed into batch.\n",
    "            Expected that each list sample will be a tuple of (text_tokens, label_tokens).\n",
    "\n",
    "    Returns:\n",
    "        Padded and packed into batch text tokens and padded and packed into batch label tokens.\n",
    "    \"\"\"\n",
    "    return seq_collate_fn(batch, TOK_PAD_ID, NER_PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a98db-01a0-4fbd-be8a-b8fa338f1a9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e50a98db-01a0-4fbd-be8a-b8fa338f1a9c",
    "outputId": "ac03e382-551e-402e-afcf-12665002ebad"
   },
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(dataset[\"train\"], vocab)  # –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "validation_dataset = NERDataset(dataset[\"validation\"], vocab)  # –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó\n",
    "test_dataset = NERDataset(dataset[\"test\"], vocab)  # –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è\n",
    "\n",
    "len(train_dataset), len(validation_dataset), len(test_dataset)  # –≤–∏–≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑—Ä–∞–∑–∫—ñ–≤ —É –∫–æ–∂–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a341a-6b1c-4b76-93cb-47d1aacc2537",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "731a341a-6b1c-4b76-93cb-47d1aacc2537",
    "outputId": "17a7e931-da7e-42bc-f670-d743324be9a1"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=ner_collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=ner_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=ner_collate_fn)\n",
    "\n",
    "len(train_loader), len(validation_loader), len(test_loader)  # –≤–∏–≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø–∞–∫–µ—Ç—ñ–≤ —É –∫–æ–∂–Ω–æ–º—É –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ –¥–∞–Ω–∏—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b641c-219a-4803-86c0-a2f7cc06bcb5",
   "metadata": {
    "id": "681b641c-219a-4803-86c0-a2f7cc06bcb5"
   },
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t3X87ZY_BKrr",
   "metadata": {
    "id": "t3X87ZY_BKrr"
   },
   "outputs": [],
   "source": [
    "def sequence_f1(true_labels: np.array, predicted_labels: np.array) -> np.array:\n",
    "    \"\"\"F1 score for one sequence.\n",
    "\n",
    "    Args:\n",
    "        true_labels: ground truth labels.\n",
    "        predicted_labels: model predictions.\n",
    "\n",
    "    Returns:\n",
    "        F1 scores for each class.\n",
    "    \"\"\"\n",
    "    assert len(true_labels) == len(predicted_labels), \"Mismatched length between true labels and predicted labels\"\n",
    "\n",
    "    scores = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å F1 –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É\n",
    "    for _cls in targets:  # –ø—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–º –∫–ª–∞—Å–∞–º –º—ñ—Ç–æ–∫\n",
    "        # –æ–±—á–∏—Å–ª—é—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å true positives, false positives —Ç–∞ false negatives –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∫–ª–∞—Å—É\n",
    "        true_positives = np.sum((true_labels == predicted_labels) & (true_labels == _cls))\n",
    "        false_positives = np.sum((true_labels != predicted_labels) & (predicted_labels == _cls))\n",
    "        false_negatives = np.sum((true_labels != predicted_labels) & (true_labels == _cls))\n",
    "\n",
    "        # –æ–±—á–∏—Å–ª—é—î–º–æ —Ç–æ—á–Ω—ñ—Å—Ç—å, –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–∞ F1 –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∫–ª–∞—Å—É\n",
    "        precision = np.nan_to_num(true_positives / (true_positives + false_positives), nan=0.0)\n",
    "        recall = np.nan_to_num(true_positives / (true_positives + false_negatives), nan=0.0)\n",
    "        f1_score = np.nan_to_num(2 * (precision * recall) / (precision + recall), nan=0.0)\n",
    "\n",
    "        scores.append(f1_score)  # –¥–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è F1 –¥–æ —Å–ø–∏—Å–∫—É\n",
    "    return np.array(scores)  # –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –º–∞—Å–∏–≤ –∑—ñ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ F1 –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdea345-fd99-4835-b15c-3b0d8b46c08a",
   "metadata": {
    "id": "bcdea345-fd99-4835-b15c-3b0d8b46c08a"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: str = \"cpu\",\n",
    "    verbose: bool = True,\n",
    ") -> Mapping[str, np.array]:\n",
    "    \"\"\"Train model one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: model to train.\n",
    "        loader: dataloader to use for training.\n",
    "        criterion: loss function to optimize.\n",
    "        optimizer: model training algorithm.\n",
    "        device: device to use for training.\n",
    "            Default is `\"cpu\"`.\n",
    "        verbose: option to print training progress bar.\n",
    "            Default is `True`.\n",
    "\n",
    "    Returns:\n",
    "        dict with training logs\n",
    "    \"\"\"\n",
    "    model.train()  # –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —É —Ä–µ–∂–∏–º —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\n",
    "\n",
    "    losses = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –≤—Ç—Ä–∞—Ç\n",
    "    scores = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –º–µ—Ç—Ä–∏–∫–∏ F1\n",
    "\n",
    "    # —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É\n",
    "    with tqdm(total=len(loader), desc=\"training\", file=sys.stdout, ncols=100, disable=not verbose) as progress:\n",
    "        for x_batch, y_true in loader:  # –ø—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –∫–æ–∂–Ω–æ–º—É –ø–∞–∫–µ—Ç—É –¥–∞–Ω–∏—Ö —É –∑–∞–≥–∞–ª—å–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö\n",
    "            x_batch = x_batch.to(device)  # –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ –ø–∞–∫–µ—Ç –¥–∞–Ω–∏—Ö –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π\n",
    "            y_true = y_true.to(device)    # –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ –º—ñ—Ç–∫–∏ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π\n",
    "\n",
    "            optimizer.zero_grad()  # –æ–±–Ω—É–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤\n",
    "\n",
    "            log_prob = model(x_batch)  # –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ\n",
    "\n",
    "            B, T = y_true.shape  # –æ—Ç—Ä–∏–º—É—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –±–∞—Ç—á—É —Ç–∞ –¥–æ–≤–∂–∏–Ω–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ\n",
    "            loss = criterion(log_prob.view(B * T, -1), y_true.view(B * T))  # –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç\n",
    "\n",
    "            loss.backward()  # –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤\n",
    "            losses.append(loss.item())  # –¥–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç –¥–æ —Å–ø–∏—Å–∫—É\n",
    "\n",
    "            # –æ—Ç—Ä–∏–º—É—î–º–æ –º–∞—Å–∏–≤–∏ NumPy –¥–ª—è –º—ñ—Ç–æ–∫ —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å\n",
    "            y_pred = log_prob.argmax(2).detach().cpu().numpy()\n",
    "            y_true = y_true.detach().cpu().numpy()\n",
    "\n",
    "            # –æ–±—á–∏—Å–ª—é—î–º–æ –º–µ—Ç—Ä–∏–∫—É F1 –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞ –≤ –ø–∞–∫–µ—Ç—ñ\n",
    "            padding_mask = y_true != NER_PAD_ID\n",
    "            for i in range(x_batch.size(0)):\n",
    "                scores.append(sequence_f1(y_true[i][padding_mask[i]], y_pred[i][padding_mask[i]]))\n",
    "\n",
    "            # –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó —É –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—ñ\n",
    "            progress.set_postfix_str(f\"loss {losses[-1]:.4f}\")\n",
    "\n",
    "            optimizer.step()  # –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–¥–µ–ª—ñ\n",
    "\n",
    "            progress.update(1)  # –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É —É –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—ñ\n",
    "\n",
    "    logs = {\n",
    "        \"losses\": np.array(losses),  # –∑–Ω–∞—á–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç\n",
    "        \"f1\": np.array(scores)        # –∑–Ω–∞—á–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫–∏ F1\n",
    "    }\n",
    "    return logs  # –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –∂—É—Ä–Ω–∞–ª —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —É —Ñ–æ—Ä–º—ñ —Å–ª–æ–≤–Ω–∏–∫–∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ee906-9afd-494a-a8ec-78bd94117a34",
   "metadata": {
    "id": "ef5ee906-9afd-494a-a8ec-78bd94117a34"
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: str = \"cpu\",\n",
    "    verbose: bool = True,\n",
    ") -> Mapping[str, np.array]:\n",
    "    \"\"\"Model evaluation.\n",
    "\n",
    "    Args:\n",
    "        model: model to evaluate.\n",
    "        loader: dataloader to use for evaluation.\n",
    "        criterion: loss function.\n",
    "        device: device to use for evaluation.\n",
    "            Default is `\"cpu\"`.\n",
    "        verbose: option to print evaluation progress bar.\n",
    "            Default is `True`.\n",
    "\n",
    "    Returns:\n",
    "        dict with evaluation logs\n",
    "    \"\"\"\n",
    "    model.eval()  # –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —É —Ä–µ–∂–∏–º –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è\n",
    "\n",
    "    losses = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –≤—Ç—Ä–∞—Ç\n",
    "    scores = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –º–µ—Ç—Ä–∏–∫–∏ F1\n",
    "\n",
    "    # —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É\n",
    "    for x_batch, y_true in tqdm(loader, desc=\"evaluation\", file=sys.stdout, ncols=100, disable=not verbose):\n",
    "        x_batch = x_batch.to(device)  # –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ –ø–∞–∫–µ—Ç –¥–∞–Ω–∏—Ö –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π\n",
    "        y_true = y_true.to(device)    # –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ –º—ñ—Ç–∫–∏ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π\n",
    "\n",
    "        log_prob = model(x_batch)  # –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ\n",
    "\n",
    "        B, T = y_true.shape  # –æ—Ç—Ä–∏–º—É—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –±–∞—Ç—á—É —Ç–∞ –¥–æ–≤–∂–∏–Ω–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ\n",
    "        loss = criterion(log_prob.view(B * T, -1), y_true.view(B * T))  # –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç\n",
    "\n",
    "        losses.append(loss.item())  # –¥–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç –¥–æ —Å–ø–∏—Å–∫—É\n",
    "\n",
    "        # –æ—Ç—Ä–∏–º—É—î–º–æ –º–∞—Å–∏–≤–∏ NumPy –¥–ª—è –º—ñ—Ç–æ–∫ —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å\n",
    "        y_pred = log_prob.argmax(2).detach().cpu().numpy()\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "\n",
    "        # –æ–±—á–∏—Å–ª—é—î–º–æ –º–µ—Ç—Ä–∏–∫—É F1 –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞ –≤ –ø–∞–∫–µ—Ç—ñ\n",
    "        padding_mask = y_true != NER_PAD_ID\n",
    "        for i in range(x_batch.size(0)):\n",
    "            scores.append(sequence_f1(y_true[i][padding_mask[i]], y_pred[i][padding_mask[i]]))\n",
    "\n",
    "    logs = {\n",
    "        \"losses\": np.array(losses),  # –∑–Ω–∞—á–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç\n",
    "        \"f1\": np.array(scores)        # –∑–Ω–∞—á–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫–∏ F1\n",
    "    }\n",
    "    return logs  # –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –∂—É—Ä–Ω–∞–ª –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è —É —Ñ–æ—Ä–º—ñ —Å–ª–æ–≤–Ω–∏–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f67db-36bd-4f2b-8c3a-c1e6b9423d09",
   "metadata": {
    "id": "8a1f67db-36bd-4f2b-8c3a-c1e6b9423d09"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f102c-86bb-4535-a87b-f1b80ac659d3",
   "metadata": {
    "id": "780f102c-86bb-4535-a87b-f1b80ac659d3"
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO: play with architecture to achieve a better score\n",
    "#######################################################################\n",
    "\n",
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, num_hidden_layers, num_classes):\n",
    "        super(NER_RNN, self).__init__()\n",
    "        # mapping from token_id to its vector representation\n",
    "        self.embed = nn.Embedding(vocab_size, input_size, padding_idx=TOK_PAD_ID)\n",
    "        # some RNN, could be nn.RNN, nn.LSTM, nn.GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size, hidden_size, num_hidden_layers, bidirectional=True, dropout=0.2, batch_first=True\n",
    "        )\n",
    "        # norm layer\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)   # * 2 because of `bidirectional=True`\n",
    "        # classification head\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # * 2 because of `bidirectional=True`\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # –≤–±—É–¥–æ–≤—É–≤–∞–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "        x, _ = self.rnn(x)  # —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω–∏–π —à–∞—Ä\n",
    "        x = self.layer_norm(x)  # –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —à–∞—Ä—É\n",
    "        x = F.relu(x)  # —Ñ—É–Ω–∫—Ü—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó ReLU\n",
    "        x = self.fc(x)  # –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∏–π —à–∞—Ä\n",
    "        scores = torch.log_softmax(x, dim=2)  # –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–≤–∞–Ω–∏—Ö –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π –∫–ª–∞—Å—ñ–≤\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e4cb8-42ad-4745-89a6-7b4ec4aa42df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c86e4cb8-42ad-4745-89a6-7b4ec4aa42df",
    "outputId": "f3975a17-2be8-4164-dd0d-635693027f3a"
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO: add learning rate scheduler\n",
    "#######################################################################\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∏—Å—Ç—Ä–æ—é –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω—å\n",
    "print(f\"Device - {device}\")\n",
    "\n",
    "torch.manual_seed(42)  # –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
    "model = NER_RNN(len(vocab), 512, 512, 3, len(targets))  # —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ\n",
    "model = model.to(device)  # –ø–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π\n",
    "print(model)\n",
    "print(\"Number of trainable parameters -\", sum(p.numel() for p in model.parameters() if p.requires_grad))  # –≤–∏–≤–µ–¥–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–¥–µ–ª—ñ, —è–∫—ñ –ø—ñ–¥–ª—è–≥–∞—é—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—é\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)  # –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫—Ä–∏—Ç–µ—Ä—ñ—é (—Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞ –∑ –ø–æ—á–∞—Ç–∫–æ–≤–æ—é —à–≤–∏–¥–∫—ñ—Å—Ç—é –Ω–∞–≤—á–∞–Ω–Ω—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe6462-2f13-4f5f-8d9c-efc24d657f7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7fe6462-2f13-4f5f-8d9c-efc24d657f7b",
    "outputId": "fcb14961-d8ca-402c-8aaa-507bbba3f4b9"
   },
   "outputs": [],
   "source": [
    "n_epochs = 30  # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "\n",
    "train_losses = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤—Ç—Ä–∞—Ç –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
    "train_scores = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –º–µ—Ç—Ä–∏–∫–∏ F1 –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
    "\n",
    "valid_losses = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤—Ç—Ä–∞—Ç –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
    "valid_scores = []  # —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –º–µ—Ç—Ä–∏–∫–∏ F1 –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
    "\n",
    "best_score = float(\"-inf\")  # –ø–æ—á–∞—Ç–∫–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫—Ä–∞—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
    "\n",
    "# —Ü–∏–∫–ª –ø–æ –µ–ø–æ—Ö–∞–º –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "for ep in range(n_epochs):\n",
    "    print(f\"\\nEpoch {ep + 1:2d}/{n_epochs:2d}\")\n",
    "\n",
    "    # –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
    "    train_logs = train_one_epoch(model, train_loader, criterion, optimizer, device, verbose=True)\n",
    "    train_losses.append(np.mean(train_logs[\"losses\"]))\n",
    "    train_scores.append(np.mean(train_logs[\"f1\"], 0))\n",
    "    print(\"      loss:\", train_losses[-1])\n",
    "    print(\"        f1:\", train_scores[-1])\n",
    "\n",
    "    # –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
    "    valid_logs = evaluate(model, validation_loader, criterion, device, verbose=True)\n",
    "    valid_losses.append(np.mean(valid_logs[\"losses\"]))\n",
    "    valid_scores.append(np.mean(valid_logs[\"f1\"], 0))\n",
    "    print(\"      loss:\", valid_losses[-1])\n",
    "    print(\"        f1:\", valid_scores[-1])\n",
    "\n",
    "    # –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ —Å—Ç–∞–Ω—É –º–æ–¥–µ–ª—ñ\n",
    "    if valid_scores[-1].mean() >= best_score:\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"epoch\": ep,\n",
    "            \"num_epochs\": n_epochs,\n",
    "            \"metrics\": {\n",
    "                \"training\": {\"loss\": train_losses[-1], \"accuracy\": train_scores[-1]},\n",
    "                \"validation\": {\"loss\": valid_losses[-1], \"accuracy\": valid_scores[-1]},\n",
    "            },\n",
    "        }\n",
    "        torch.save(checkpoint, \"best.pth\")\n",
    "        print(\"üü¢ Saved new best state! üü¢\")\n",
    "        best_score = valid_scores[-1].mean()  # –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∫—Ä–∞—â–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –¥–æ –Ω–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aac7a1-7546-4c8c-86ad-86708d2a02b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "37aac7a1-7546-4c8c-86ad-86708d2a02b8",
    "outputId": "e6fe71bc-0a77-4c12-f98c-a6046f0f7f00"
   },
   "outputs": [],
   "source": [
    "# NOTE: plot training and validation performance\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(22, 6))\n",
    "\n",
    "axes[0].plot(np.arange(len(train_losses)), train_losses, \".-\")\n",
    "axes[0].plot(np.arange(len(valid_losses)), valid_losses, \".-\")\n",
    "axes[0].legend([\"train\", \"validation\"])\n",
    "axes[0].set_title(\"Loss\")\n",
    "axes[0].grid()\n",
    "\n",
    "axes[1].plot(np.arange(len(train_scores)), [item.mean() for item in train_scores], \".-\")\n",
    "axes[1].plot(np.arange(len(valid_scores)), [item.mean() for item in valid_scores], \".-\")\n",
    "axes[1].legend([\"train\", \"validation\"])\n",
    "axes[1].set_title(\"F1\")\n",
    "axes[1].grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T_mfdkqoMKIh",
   "metadata": {
    "id": "T_mfdkqoMKIh"
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO: load model best state\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27190acf-4e1d-41b8-bdea-8f73b9239351",
   "metadata": {
    "id": "27190acf-4e1d-41b8-bdea-8f73b9239351"
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO: write evaluation function for a test set.\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eLyidJutRIvr",
   "metadata": {
    "id": "eLyidJutRIvr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
