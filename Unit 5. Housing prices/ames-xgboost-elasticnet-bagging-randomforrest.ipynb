{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Disclaimer\nThis is my first Kaggle submission. While I hope my work can be useful, please note that some steps may appear non-standard or unconventional. Any seemingly “weird” decisions were likely made intentionally based on the underlying logic and problem context.\n\n---\n\n# Notebook Overview\n\nIn this notebook, I perform the following key steps:\n\n- **Basic Exploratory Data Analysis (EDA):**\n  High-level examinination of features to uncover missing values, data types in the data.\n- **Feature Engineering:**\n    Focused on handling of missing values using group-based imputation methods, logical transformations, and categorical encoding to capture the essence of the dataset.\n- **Modeling and Hyperparameter Tuning:**\n    Implemented several models – including `XGBoost`, `ElasticNet`, `Bagging Regressor`, and `Random Forest` – and optimized them using `GridSearchCV` with `KFold cross validation`. <br>\n    **Training Metric:** `Root Mean Squared Error (RMSE)` applied to the logarithm of the predicted and observed sale prices.","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn==1.2.2   # xgboost compatibility issue (February 2025) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:30:56.486288Z","iopub.execute_input":"2025-02-05T06:30:56.486734Z","iopub.status.idle":"2025-02-05T06:31:00.784858Z","shell.execute_reply.started":"2025-02-05T06:30:56.486700Z","shell.execute_reply":"2025-02-05T06:31:00.783682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\n\nfrom copy import deepcopy\nfrom pandas.api.types import is_numeric_dtype\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.786614Z","iopub.execute_input":"2025-02-05T06:31:00.787014Z","iopub.status.idle":"2025-02-05T06:31:00.793594Z","shell.execute_reply.started":"2025-02-05T06:31:00.786981Z","shell.execute_reply":"2025-02-05T06:31:00.792447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_raw = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_raw = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.795456Z","iopub.execute_input":"2025-02-05T06:31:00.795826Z","iopub.status.idle":"2025-02-05T06:31:00.868093Z","shell.execute_reply.started":"2025-02-05T06:31:00.795796Z","shell.execute_reply":"2025-02-05T06:31:00.866980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train = train_raw[['Id', 'SalePrice']].copy()\ny_train['SalePrice'] = np.log1p(y_train['SalePrice'])\n\nx_id = test_raw['Id'].copy()\ny_train\ntrain_features=train_raw.drop('SalePrice', axis=1)\ncombined_raw = pd.concat([train_features, test_raw], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.869570Z","iopub.execute_input":"2025-02-05T06:31:00.869892Z","iopub.status.idle":"2025-02-05T06:31:00.887222Z","shell.execute_reply.started":"2025-02-05T06:31:00.869865Z","shell.execute_reply":"2025-02-05T06:31:00.885973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"print(\"*** combined_raw\")\n\nprint(f\"--- Shape of the dataset: {combined_raw.shape}\\n\")\nprint(f\"--- Null values:\")\ncombined_null_stats = combined_raw.isnull().sum()\nprint(combined_null_stats[combined_null_stats > 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.888375Z","iopub.execute_input":"2025-02-05T06:31:00.888729Z","iopub.status.idle":"2025-02-05T06:31:00.909633Z","shell.execute_reply.started":"2025-02-05T06:31:00.888698Z","shell.execute_reply":"2025-02-05T06:31:00.908632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"--- Dataset info:\\n{combined_raw.info()}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.910588Z","iopub.execute_input":"2025-02-05T06:31:00.910875Z","iopub.status.idle":"2025-02-05T06:31:00.941751Z","shell.execute_reply.started":"2025-02-05T06:31:00.910851Z","shell.execute_reply":"2025-02-05T06:31:00.940710Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cleanup Functions Declaration\n\nThe key idea behind data cleanup was to fill the missing values with as much meaningful information as possible & dropping the low-information columns.\n\nGeneral and group-specific rules (garage/basement) were applied. The data was also transformed into numerical format.","metadata":{}},{"cell_type":"markdown","source":"The following function, `fillna_and_transform_numeric`, is designed to handle missing values in `numeric features` by performing median imputation within groups. This approach leverages the idea that similar observations (as defined by one or more grouping features) tend to have similar values for a given numeric feature.","metadata":{}},{"cell_type":"code","source":"def fillna_and_transform_numeric(dataframe, numeric_features, grouping_features):\n\n    for feature in numeric_features:\n        # First try to fill using the full grouping_features list\n        dataframe[feature] = dataframe[feature].fillna(\n            dataframe.groupby(grouping_features)[feature].transform('median')\n        )\n        # If many are still missing, fall back to grouping by the first grouping_feature only\n        if dataframe[feature].isnull().sum() > 1:\n            dataframe[feature] = dataframe[feature].fillna(\n                dataframe.groupby(grouping_features[0])[feature].transform('median')\n            )\n\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.942705Z","iopub.execute_input":"2025-02-05T06:31:00.942988Z","iopub.status.idle":"2025-02-05T06:31:00.948350Z","shell.execute_reply.started":"2025-02-05T06:31:00.942963Z","shell.execute_reply":"2025-02-05T06:31:00.947298Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function, `fillna_and_transform_categorical`, processes `categorical columns` by first converting non-missing category labels into integers and then imputing missing values based on the most frequent category (mode) within similar groups.","metadata":{}},{"cell_type":"code","source":"def fillna_and_transform_categorical(dataframe, categorical_features, grouping_features):\n\n    for feature in categorical_features:\n        # Map non-missing categories to integers\n        unique_categories = dataframe[feature].dropna().unique()\n        category_mapping = {cat: idx for idx, cat in enumerate(unique_categories, start=1)}\n        dataframe[feature] = dataframe[feature].map(category_mapping)\n\n        # Fill missing values groupwise by mode\n        dataframe[feature] = dataframe.groupby(grouping_features)[feature].transform(\n            lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 0)\n        )\n        # Fallback using grouping by the first grouping feature if still missing\n        if dataframe[feature].isnull().sum() > 1:\n            dataframe[feature] = dataframe.groupby(grouping_features[0])[feature].transform(\n                lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 0)\n            )\n\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.951465Z","iopub.execute_input":"2025-02-05T06:31:00.951786Z","iopub.status.idle":"2025-02-05T06:31:00.969931Z","shell.execute_reply.started":"2025-02-05T06:31:00.951760Z","shell.execute_reply":"2025-02-05T06:31:00.968858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The function `fillna_and_transform_provided_zero_features` is designed to process a set of features (specified in the `to_zero_features list`) by ensuring that they contain no missing values.","metadata":{}},{"cell_type":"code","source":"def fillna_and_transform_provided_zero_features(dataframe, to_zero_features):\n\n    for feature in to_zero_features:\n        # If feature is not numeric, first map unique categories to integers.\n        if not is_numeric_dtype(dataframe[feature]):\n            unique_categories = dataframe[feature].dropna().unique()\n            category_mapping = {cat: idx for idx, cat in enumerate(unique_categories, start=1)}\n            dataframe[feature] = dataframe[feature].map(category_mapping)\n        # Finally, fill any missing values with 0.\n        dataframe[feature] = dataframe[feature].fillna(0)\n\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.971512Z","iopub.execute_input":"2025-02-05T06:31:00.971817Z","iopub.status.idle":"2025-02-05T06:31:00.984220Z","shell.execute_reply.started":"2025-02-05T06:31:00.971792Z","shell.execute_reply":"2025-02-05T06:31:00.983218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function, `fillna_and_transform_basement`, is designed to handle missing values for `basement-related features` in a dataset. It treats numeric and categorical basement columns separately and uses the key column (`TotalBsmtSF`) to guide the imputation logic.","metadata":{}},{"cell_type":"code","source":"def fillna_and_transform_basement(dataframe, basement_numeric_cols, basement_categorical_cols, grouping_features, key_col):\n    # Ensure key col (e.g. TotalBsmtSF) has no NAs\n    dataframe[key_col] = dataframe[key_col].fillna(0)\n\n    # Create masks for rows with and without basement info.\n    mask_no_basement = dataframe[key_col] == 0\n    mask_with_basement = ~mask_no_basement\n\n    # Case 1: For rows with no basement info, set both numeric and categorical columns to 0.\n    dataframe.loc[mask_no_basement, basement_numeric_cols] = 0\n    dataframe.loc[mask_no_basement, basement_categorical_cols] = 0\n\n    # Case 2: For rows with basement info\n    # For numeric columns: fill missing values with an equal split of the deficit.\n    def fill_numeric_basement(row):\n        if row[key_col] > 0:\n            missing = [col for col in basement_numeric_cols if pd.isna(row[col])]\n            if missing:\n                known_sum = row[basement_numeric_cols].sum(skipna=True)\n                deficit = row[key_col] - known_sum\n                if deficit < 0:\n                    deficit = 0\n                fill_value = deficit / len(missing)\n                for col in missing:\n                    row[col] = fill_value\n        return row\n\n    dataframe.loc[mask_with_basement, :] = dataframe.loc[mask_with_basement, :].apply(fill_numeric_basement, axis=1)\n\n    # For categorical columns: apply mapping and groupwise fill only to rows with basement info.\n    for col in basement_categorical_cols:\n        # Only consider rows with basement (mask_with_basement) and non-zero, non-null values.\n        non_missing = dataframe.loc[mask_with_basement & dataframe[col].notna() & (dataframe[col] != 0), col].unique()\n        category_mapping = {cat: idx for idx, cat in enumerate(non_missing, start=1)}\n\n        # Apply mapping only on rows with basement info.\n        dataframe.loc[mask_with_basement, col] = dataframe.loc[mask_with_basement, col].map(category_mapping)\n\n        # Groupwise fill missing values using mode (only among rows with basement info).\n        dataframe.loc[mask_with_basement, col] = dataframe.loc[mask_with_basement].groupby(grouping_features)[col].transform(\n            lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 0)\n        )\n\n        # Any remaining missing values in rows with basement are set to 0.\n        dataframe.loc[mask_with_basement, col] = dataframe.loc[mask_with_basement, col].fillna(0)\n\n    return dataframe\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:00.985227Z","iopub.execute_input":"2025-02-05T06:31:00.985518Z","iopub.status.idle":"2025-02-05T06:31:01.001849Z","shell.execute_reply.started":"2025-02-05T06:31:00.985493Z","shell.execute_reply":"2025-02-05T06:31:01.000897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The function `fillna_and_transform_garage` is designed to handle missing values and standardize the garage-related features in the dataset. It uses the key column (`GarageType`) to split the data into rows with and without a garage, then `imputes or maps features accordingly`.","metadata":{}},{"cell_type":"code","source":"def fillna_and_transform_garage(dataframe, garage_cols, key_col):\n    # Replace missing GarageType with 0 and create masks\n    dataframe[key_col] = dataframe[key_col].fillna(0)\n    mask_no_garage = dataframe[key_col] == 0\n    mask_garage = ~mask_no_garage\n\n    # For rows without GarageType (no garage info), set all garage-related columns to 0.\n    dataframe.loc[mask_no_garage, garage_cols] = 0\n\n    # For rows with GarageType information, impute numeric values.\n    # Impute 'GarageCars' using mode within each GarageType group.\n    mode_by_type = dataframe.loc[mask_garage].groupby(key_col)['GarageCars'].agg(\n        lambda x: x.mode().iloc[0] if not x.mode().empty else 0\n    )\n    def impute_garage_cars(row):\n        if pd.isna(row['GarageCars']):\n            return mode_by_type.get(row[key_col], 0)\n        return row['GarageCars']\n    dataframe.loc[mask_garage, 'GarageCars'] = dataframe.loc[mask_garage].apply(impute_garage_cars, axis=1)\n\n    # Impute 'GarageArea' using median within each GarageType group.\n    median_by_type = dataframe.loc[mask_garage].groupby(key_col)['GarageArea'].median()\n    def impute_garage_area(row):\n        if pd.isna(row['GarageArea']):\n            return median_by_type.get(row[key_col], 0)\n        return row['GarageArea']\n    dataframe.loc[mask_garage, 'GarageArea'] = dataframe.loc[mask_garage].apply(impute_garage_area, axis=1)\n\n    # Process remaining categorical garage columns: 'GarageFinish', 'GarageQual', 'GarageCond'\n    cat_cols = ['GarageFinish', 'GarageQual', 'GarageCond']\n    for col in cat_cols:\n        # Restrict processing to rows with garage information; exclude 0 from unique values.\n        non_missing = dataframe.loc[mask_garage & (dataframe[col] != 0) & (dataframe[col].notna()), col].unique()\n        mapping = {cat: idx for idx, cat in enumerate(non_missing, start=1)}\n        dataframe.loc[mask_garage, col] = dataframe.loc[mask_garage, col].map(mapping)\n\n        # Within each GarageType group, fill missing values in the column using mode.\n        dataframe.loc[mask_garage, col] = dataframe.loc[mask_garage].groupby(key_col)[col].transform(\n            lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 0)\n        )\n        # Any leftover missing values are set to 0.\n        dataframe.loc[mask_garage, col] = dataframe.loc[mask_garage, col].fillna(0)\n\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:01.002731Z","iopub.execute_input":"2025-02-05T06:31:01.002998Z","iopub.status.idle":"2025-02-05T06:31:01.022768Z","shell.execute_reply.started":"2025-02-05T06:31:01.002976Z","shell.execute_reply":"2025-02-05T06:31:01.021791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The function `transform_categorical_grouping_features` converts the specified grouping features from `categorical labels into numeric codes`.","metadata":{}},{"cell_type":"code","source":"def transform_categorical_grouping_features(dataframe, grouping_features):\n    for feature in grouping_features:\n        unique_categories = dataframe[feature].dropna().unique()\n        mapping = {cat: idx for idx, cat in enumerate(unique_categories, start=1)}\n        dataframe[feature] = dataframe[feature].map(mapping)\n    return dataframe\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:01.023692Z","iopub.execute_input":"2025-02-05T06:31:01.023992Z","iopub.status.idle":"2025-02-05T06:31:01.042384Z","shell.execute_reply.started":"2025-02-05T06:31:01.023968Z","shell.execute_reply":"2025-02-05T06:31:01.041451Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The function `drop_low_information_features` is designed to remove columns that are deemed to have `low information content`.","metadata":{}},{"cell_type":"code","source":"def drop_low_information_features(dataframe, features_to_drop):\n    dataframe.drop(features_to_drop, axis=1, inplace=True)\n\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:01.043321Z","iopub.execute_input":"2025-02-05T06:31:01.043659Z","iopub.status.idle":"2025-02-05T06:31:01.058956Z","shell.execute_reply.started":"2025-02-05T06:31:01.043632Z","shell.execute_reply":"2025-02-05T06:31:01.057925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_transformation(df):\n\n  grouping_features=['Neighborhood', 'MSSubClass']\n  grouping_keys_for_transform = ['GarageType']\n\n  #\n\n  to_zero_features = ['MasVnrArea','FireplaceQu', 'GarageYrBlt', 'PoolQC',\n                      'MiscFeature','Alley','Fence','MasVnrType']\n\n  numeric_features = ['LotFrontage']\n\n  categorical_features = ['MSZoning', 'Exterior1st', 'Exterior2nd', 'KitchenQual',\n                          'Utilities', 'Functional', 'SaleType', 'GarageType', 'Street',\n                          'LotShape', 'LandContour', 'LotConfig', 'LandSlope',\n                          'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n                          'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation',\n                          'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n                          'PavedDrive', 'SaleCondition']\n\n  basement_numeric_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath']\n  basement_categorical_cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n  key_col_basement = 'TotalBsmtSF'\n\n  garage_cols = ['GarageCars', 'GarageArea', 'GarageFinish', 'GarageQual', 'GarageCond']\n  key_col_garage = 'GarageType'\n\n  #\n\n  df_transformed = deepcopy(df)\n\n  df_transformed = fillna_and_transform_provided_zero_features(df_transformed, to_zero_features)\n  df_transformed = fillna_and_transform_numeric(df_transformed, numeric_features, grouping_features)\n  df_transformed = fillna_and_transform_categorical(df_transformed, categorical_features, grouping_features)\n\n  df_transformed = fillna_and_transform_basement(df_transformed, basement_numeric_cols,\n                                basement_categorical_cols, grouping_features, key_col_basement)\n\n  df_transformed = fillna_and_transform_garage(df_transformed, garage_cols, key_col_garage)\n\n  df_transformed = transform_categorical_grouping_features(df_transformed, grouping_features)\n  df_transformed = transform_categorical_grouping_features(df_transformed, grouping_keys_for_transform)\n\n  for feature in df_transformed.columns:\n    df_transformed[feature] = df_transformed[feature].astype('int64')\n\n  return df_transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:01.059839Z","iopub.execute_input":"2025-02-05T06:31:01.060115Z","iopub.status.idle":"2025-02-05T06:31:01.075741Z","shell.execute_reply.started":"2025-02-05T06:31:01.060085Z","shell.execute_reply":"2025-02-05T06:31:01.074765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Combined Dataframe : Employing Cleanup","metadata":{}},{"cell_type":"code","source":"combined = data_transformation(combined_raw)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:01.076547Z","iopub.execute_input":"2025-02-05T06:31:01.076810Z","iopub.status.idle":"2025-02-05T06:31:03.781448Z","shell.execute_reply.started":"2025-02-05T06:31:01.076787Z","shell.execute_reply":"2025-02-05T06:31:03.780333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"*** combined\")\n\nprint(f\"--- Shape of the dataset:{combined.shape}\")\nprint(f\"--- Null values: \", end=\"\")\ntest_null_stats = combined.isnull().sum()\nprint(test_null_stats[test_null_stats > 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:03.782632Z","iopub.execute_input":"2025-02-05T06:31:03.783030Z","iopub.status.idle":"2025-02-05T06:31:03.794361Z","shell.execute_reply.started":"2025-02-05T06:31:03.782992Z","shell.execute_reply":"2025-02-05T06:31:03.793190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = combined[combined['Id'].isin(y_train['Id'])].copy()\nX_train.drop(['Id'],axis=1,inplace=True)\nX_test = combined[~combined['Id'].isin(y_train['Id'])].copy()\nX_test.drop(['Id'],axis=1,inplace=True)\n\ny_train.drop(['Id'], axis=1,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:03.795565Z","iopub.execute_input":"2025-02-05T06:31:03.795836Z","iopub.status.idle":"2025-02-05T06:31:03.826428Z","shell.execute_reply.started":"2025-02-05T06:31:03.795812Z","shell.execute_reply":"2025-02-05T06:31:03.825655Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models & Hyperparameters Declaration","metadata":{}},{"cell_type":"code","source":"model_dict = {\n    'xgb': {\n        'model': xgb.XGBRegressor(random_state=42),\n        'params': {\n            'n_estimators': [100, 200, 300, 400, 500],\n            'learning_rate': [0.04],\n            'max_depth': [2, 3, 4, 5],\n            'min_child_weight': [1, 2, 3]\n        }\n    },\n    'elasticnet': {\n        'model': ElasticNet(random_state=42, max_iter=1000),\n        'params': {\n            'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.25, 1.5],\n            'l1_ratio': [0, 0.25, 0.5, 0.75, 1]\n        }\n    },\n    'bagging': {\n        'model': BaggingRegressor(random_state=42),\n        'params': {\n            'n_estimators': [375, 400, 425],\n            'max_samples': [0.3, 0.4, 0.5]\n        }\n    },\n    'random_forest': {\n        'model': RandomForestRegressor(random_state=42),\n        'params': {\n            'n_estimators': [300, 400, 500],\n            'max_depth': [10, 15, 20, 25, 30],\n            'min_samples_split': [2, 3, 4]\n        }\n    }\n}\n\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:03.827405Z","iopub.execute_input":"2025-02-05T06:31:03.827697Z","iopub.status.idle":"2025-02-05T06:31:03.834619Z","shell.execute_reply.started":"2025-02-05T06:31:03.827673Z","shell.execute_reply":"2025-02-05T06:31:03.833552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Best Model Search","metadata":{}},{"cell_type":"code","source":"best_models = {}\nfor name, definition in model_dict.items():\n    model = definition['model']\n    param_grid = definition['params']\n\n    grid = GridSearchCV(model, param_grid, cv=cv,\n                        scoring='neg_root_mean_squared_error',\n                        n_jobs=-1)\n    grid.fit(X_train, y_train)\n\n    best_models[name] = grid\n    print(f\"Model: {name}\")\n    print(\"Best Parameters:\", grid.best_params_)\n    print(\"Best RMSE:\", -grid.best_score_)\n    print(\"--------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:31:03.835643Z","iopub.execute_input":"2025-02-05T06:31:03.835936Z","iopub.status.idle":"2025-02-05T06:43:45.458575Z","shell.execute_reply.started":"2025-02-05T06:31:03.835903Z","shell.execute_reply":"2025-02-05T06:43:45.457232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Employing Best Model on Test Dataset & Submission File Generation","metadata":{}},{"cell_type":"code","source":"best_model_name = min(best_models, key=lambda model: -best_models[model].best_score_)\n\nmodel_details = best_models[best_model_name]\n\nprint(\"Best model based on RMSE:\",best_model_name)\nprint(\"Best Parameters:\", model_details.best_params_)\nprint(\"Best RMSE:\", -model_details.best_score_)\n\nprint(\"\\nInstantiating best model...\")\n\n# Instantiate the corresponding model with the best parameters\nif best_model_name == 'xgb':\n    best_model = xgb.XGBRegressor(**model_details.best_params_)\nelif best_model_name == 'elasticnet':\n    best_model = ElasticNet(**model_details.best_params_)\nelif best_model_name == 'bagging':\n    best_model = BaggingRegressor(**model_details.best_params_)\nelif best_model_name == 'random_forest':\n    best_model = RandomForestRegressor(**model_details.best_params_)\n\nbest_model.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:45:19.334627Z","iopub.execute_input":"2025-02-05T06:45:19.335001Z","iopub.status.idle":"2025-02-05T06:45:19.344093Z","shell.execute_reply.started":"2025-02-05T06:45:19.334973Z","shell.execute_reply":"2025-02-05T06:45:19.342968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = best_model.predict(X_test)\n\npredicted_prices = np.expm1(predictions)\n\npd.DataFrame({'Id': x_id, 'SalePrice': predicted_prices}).to_csv('predictions.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:47:58.631765Z","iopub.execute_input":"2025-02-05T06:47:58.632114Z","iopub.status.idle":"2025-02-05T06:47:58.655646Z","shell.execute_reply.started":"2025-02-05T06:47:58.632089Z","shell.execute_reply":"2025-02-05T06:47:58.654623Z"}},"outputs":[],"execution_count":null}]}